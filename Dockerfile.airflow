FROM apache/airflow:2.8.1

USER root

# Install OpenJDK 17 + Docker CLI (ADDED docker.io)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openjdk-17-jdk \
        curl \
        docker.io && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# ---------- ADD THIS PART ----------
# Install Spark
ENV SPARK_VERSION=3.5.3
ENV HADOOP_VERSION=3

RUN curl -L https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark

ENV SPARK_HOME=/opt/spark
ENV PATH="${SPARK_HOME}/bin:${PATH}"
# ---------- END ADDITION ----------

COPY spark-jars/*.jar /opt/spark/jars/

# Create data directory
RUN mkdir -p /opt/airflow/data && \
    chown -R airflow:root /opt/airflow/data && \
    chmod -R 775 /opt/airflow/data

USER airflow

# Install Python packages
COPY airflow/requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt
